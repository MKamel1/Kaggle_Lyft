{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet50\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.evaluation import write_pred_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "rc('animation', html='jshtml')\n",
    "from IPython.display import HTML\n",
    "#from IPython.display import display, clear_output\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import zarr\n",
    "import itertools as it\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<zarr.core.Array '/frames' (24838,) [('timestamp', '<i8'), ('agent_index_interval', '<i8', (2,)), ('traffic_light_faces_index_interval', '<i8', (2,)), ('ego_translation', '<f8', (3,)), ('ego_rotation', '<f8', (3, 3))] read-only>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"<zarr.core.Array '/agents' (1893736,) [('centroid', '<f8', (2,)), ('extent', '<f4', (3,)), ('yaw', '<f4'), ('velocity', '<f4', (2,)), ('track_id', '<u8'), ('label_probabilities', '<f4', (17,))] read-only>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"<zarr.core.Array '/scenes' (100,) [('frame_index_interval', '<i8', (2,)), ('host', '<U16'), ('start_time', '<i8'), ('end_time', '<i8')] read-only>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"<zarr.core.Array '/traffic_light_faces' (316008,) [('face_id', '<U16'), ('traffic_light_id', '<U16'), ('traffic_light_face_status', '<f4', (3,))] read-only>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"data\"\n",
    "# cfg = load_config_data(\"examples/visualisation/visualisation_config.yaml\")\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "    \n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "    \n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 12,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# print(cfg)\n",
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require('scenes/sample.zarr')\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "zarr_dataset.open()\n",
    "\n",
    "frames = zarr_dataset.frames\n",
    "agents = zarr_dataset.agents\n",
    "scenes = zarr_dataset.scenes\n",
    "tl_faces = zarr_dataset.tl_faces\n",
    "\n",
    "display(str(frames))\n",
    "display(str(agents))\n",
    "display(str(scenes))\n",
    "display(str(tl_faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg: Dict) -> torch.nn.Module:\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet50(pretrained=False)\n",
    "\n",
    "    # change input channels number to match the rasterizer's output\n",
    "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "    num_in_channels = 3 + num_history_channels\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_in_channels,\n",
    "        model.conv1.out_channels,\n",
    "        kernel_size=model.conv1.kernel_size,\n",
    "        stride=model.conv1.stride,\n",
    "        padding=model.conv1.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "    # change output size to (X, Y) * number of future states\n",
    "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    return model\n",
    "\n",
    "def forward(data, model, device):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: you're running with a custom agents_mask\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   11314    |  1131400   |  88594921  |    7854144    |      31.43      |        100.00        |        78.31         |        10.00         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "test_cfg = cfg[\"test_data_loader\"]\n",
    "\n",
    "test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
    "test_mask = np.load(\"data/scenes/mask.npz\")[\"arr_0\"]\n",
    "\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             shuffle=test_cfg[\"shuffle\"],\n",
    "                             batch_size=test_cfg[\"batch_size\"],\n",
    "                             num_workers=test_cfg[\"num_workers\"])\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5927 [00:00<?, ?it/s]C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\l5kit\\dataset\\agent.py:115: RuntimeWarning: disable_traffic_light_faces not found in config, this will raise an error in the future\n",
      "  return self.get_frame(scene_index, state_index, track_id=track_id)\n",
      " 28%|██▊       | 1665/5927 [51:00<7:29:58,  6.33s/it] "
     ]
    }
   ],
   "source": [
    "# ==== INIT MODEL\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"baseline_weights/baseline_weights.pth\", map_location=device))\n",
    "# ==== EVAL LOOP\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "\n",
    "progress_bar = tqdm(test_dataloader)\n",
    "for data in progress_bar:\n",
    "    outputs = forward(data, model, device).cpu().numpy().copy()\n",
    "\n",
    "    # convert into world coordinates offsets\n",
    "    coords_offset = []\n",
    "    for agent_coords, agent_yaw_rad in zip(outputs, data[\"yaw\"]):\n",
    "        world_offset_from_agent = np.array(\n",
    "        [\n",
    "            [np.cos(agent_yaw_rad), -np.sin(agent_yaw_rad), 0],\n",
    "            [np.sin(agent_yaw_rad), np.cos(agent_yaw_rad), 0],\n",
    "            [0, 0, 1],\n",
    "        ])\n",
    "        coords_offset.append(transform_points(agent_coords, world_offset_from_agent))\n",
    "    \n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pred_csv(\"submission/submission.csv\",\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "              )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
